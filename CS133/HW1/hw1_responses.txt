CS 133 Homework 1
Dennis Zang (704766877)

1.Find out the number of cores of the processors in your cell phone and laptop/desktop.
Specify their types, if known.
	cell phone: 4 cores (Qualcomm Snapdragon MSM8917)
	laptop: 6 cores (Intel 8th Generation Core i7)

2. What is Dennard Scaling? What caused its breakdown?
	Dennard Scaling is the trend of transistors shrinking by 30% per dimension every 2 years (or transistor size halving
	each generation) while maintaining the same internal electric field (which requires scaling the voltage linearly with size).
	The breakdown of this trend is due to leakage power, since lowering the scaling would also require the lowering of threshold
	voltage. If the threshold voltage is too small, then leakage currents would increase, becoming a significant factor in
	unintended power consumption.

3.Please compute the power efficiency of the Top-10 supercomputers announced in
Nov. 2019, and list the top-3 most power efficient supercomputers. Please use the
measurement in terms of Rmax/Power (you can compute only those whose Power
numbers are available).
	Rank	System									RMax (TFlop/s)		Power (kW)		Efficiency (TFlop/kJ)
	1		Summit									148,600.0			10,096			14.719
	2		Sierra									94,640.0			7,438			12.724
	3		Sunway TaihuLight						93,014.6			15,371			6.051
	4		Tianhe-2A								61,444.5			18,482			3.325
	5		Frontera								23,516.4 			---				---
	6		Piz Daint								21,230.0 			2,384			8.905
	7		Trinity									20,158.7			7,578			2.660
	8		AI Bridging Cloud Infrastructure (ABCI)	19,880.0			1,649			12.056
	9		SuperMUC-NG								19,476.6			---				---
	10		Lassen									18,200.0			---				---
	(Top 3 most power efficient: [#1] Summit, [#2] Sierra, and [#3] AI Bridging Cloud Infrastructure)
	(source: https://www.top500.org/lists/2019/11/)

4.Given an integer array a[ ] of N elements. Please write an OpenMP function to sort it
by the Quicksort algorithm using the task directive. The function header is: void
quicksort(int *a, int p, int r). (p represents the start index and r
represents the end index)
	#include <omp.h>
	#include <queue>
	using namespace std;

	void quicksort(int *a, int p, int r)
	{
		if(p >= r)
			return;
		int mid = a[r];
		queue<int> lessThan;
		queue<int> greaterThanEquals;
		for(int i = p; i < r; i++)
		{
			if(a[i] < mid)
				lessThan.push(a[i]);
			else
				greaterThanEquals.push(a[i]);
		}
		int curr = p;
		while(!lessThan.empty())
		{
			a[curr] = lessThan.front();
			curr++;
			lessThan.pop();
		}
		a[curr] = mid;
		mid = curr;
		curr++;
		while(!greaterThanEquals.empty())
		{
			a[curr] = greaterThanEquals.front();
			curr++;
			greaterThanEquals.pop();
		}
		#pragma omp parallel
		{
			#pragma omp single
			{
				#pragma omp task
				quicksort(a, p, mid - 1);
				#pragma omp task
				quicksort(a, mid + 1, r);
			}
		}
		return;
	}

5.For the all-pair shortest path code provided in Lecture 2:
(i) Please list all data dependencies and their types; and
(ii) Please examine the following loop transformation operations discussed in
Lecture 3:
i. Loop Permutation
ii. Loop Distribution
iii. Loop Fusion
iv. Loop Peeling
v. Loop Shifting
vi. Loop Unrolling
vii. Loop Strip-Mining
viii. Loop Unroll-and-Jam
ix. Loop Tiling
x. Loop Parallelization
xi. Loop Vectorization
Please discuss which one can be applied and which one cannot be. For some
transformations, they can be applied to some loops, but not others. Please
discuss both cases.

(i) (see comments on right)
	int main()
	{
		int i, j, k;
		float **a, **b;
		... // initialize a[][], b[][] as the 1-hop distance matrix
		for (k = 0; k < N; k++)
			for (i = 0; i < N; i++)	
				for (j = 0; j < N; j++)
					a[i][j] = min(a[i][j], b[i][k] + b[k][j]);
					... // copy a[][] to b[][]
	}

	"... // initialize a[][], b[][] as the 1-hop distance matrix" has an output dependence on "float **a, **b;".
	For each iteration of k, all values of a[][] and b[][] are written to, which would be an output dependency between each iteration of
	k.
	For each iteration of k, all values of a[][], and some from b[][] (i=k, j=k, or both), are read from, and combined with the previous
	fact, this would be both a flow dependency (read-after-write between k and k-1) and an anti dependency (write-after-read between k+1
	and k).
	Between "a[i][j] = min(a[i][j], b[i][k] + b[k][j]);" and "... // copy a[][] to b[][]" in the loop body for a given (i,j,k); there is
	a flow dependency for a[][]. Note that there is theoretically no anti dependency for b[][] in the loop body for a given iteration of
	k (to write to b[i][k] or b[k][j], i=k, j=k, or both for a given k; then b[i][j] = a[i][j] and either b[i][k]=0 or b[k][j]=0,
	meaning we essentially write the same value to b[i][j]; thus, the values of b[i][k] and b[k][j] we are reading from for a given k
	cannot be written to).

(ii)
	i. Loop Permutation:
		This transformation can be applied on the for loops for i and j, as there is no interdependency between each a[][] and/or b[][]
		for a given k. Each unique pair (i,j) corresponds to a different value of a[][] and b[][], so each a[][] are independent from
		one another. Each b[][] written to for a given k will not be read from for the same iteration of k, as this is only possible if
		i=k or j=k, and in these cases, b[i][k]=0 or b[k][j]=0, and b[i][j]=a[i][j]. This basically rules out reading from and writing
		to the same value (i,j) in the same iteration of k.
		However, this transformation cannot be applied for any combination with the variable k, as doing so will basically have the loop
		body read the same value b[][] within the same iteration of the new outermost variable. 
	ii. Loop Distribution:
		This transformation cannot be applied, as we need to update b[][]'s values immediately alongside a[][]. We cannot change the
		frequency b[][] is updated without changing the behavior of the program.
	iii. Loop Fusion:
		Given that there is only a single loop body in this program, this is inapplicable here.
	iv. Loop Peeling:
		This transformation can be applied to any of the 3 loops above, however the entireity of the loop body of the loop being split
		must be duplicated entirely outside the loop for each iteration peeled for the program behavior not to change.
		(This wouldn't change the execution order of loop bodies for a single loop of the program.)
	v. Loop Shifting:
		Like loop peeling, this can be applied to any of the 3 loops above, given that the entireity of the loop body is duplicated
		before and after the loop.
		(This wouldn't change the execution order of loop bodies for a single loop of the program.)
	vi. Loop Unrolling:
		This transformation can be applied to any of the 3 loops above, given that the loop body of the modified loop is duplicated as
		many times the loop is unrolled to compensate.
		(This wouldn't change the execution order of loop bodies for a single loop of the program.)
	vii. Loop Strip-Mining:
		This transformation can be applied to any of the 3 loops above, given that the loop bounds and nested multiples of iterations
		are modified correctly to modularize the loop to the correct number of iterations.
		(This wouldn't change the execution order of loop bodies for a single loop of the program.)
	viii. Loop Unroll-and-Jam:
		This can be done for the for loops with i and j, as there is no dependence between each (i,j) for a given k. However, there are
		statement dependencies that span across values of k (each statement for a given value of k must be done in ordered "batches" to
		keep the dependency), so this transformation cannot be done for the for loop with k.
	ix. Loop Tiling:
		Like loop unroll-and-jam, this can only be performed on the for loops with i and j for the exact same reasons.
	x. Loop Parallelization:
		Loop parallelization can be performed on the for loops with i and j (no dependencies between loop bodies for a given iteration
		of k). It cannot be performed on the for loop with k due to the dependencies between each iteration of k.
	xi. Loop Vectorization:
		This transformation can be applied to any of the 3 loops above, given that the loop bounds and nested multiples of iterations
		are modified correctly to modularize the loop to the correct number of iterations.
		(This wouldn't change the execution order of loop bodies for a single loop of the program.)

6.There is a list of n independent tasks with known (but considerably different) runtimes to be
performed by m processors. We order the tasks in a list and assign each task in the order of
the list to the first available idle processor until all tasks are completed (so called the list
scheduling). Once a processor finishes a task, it requests a new task. Alice sorts the list in
decreasing order of the task runtimes and then performs list scheduling. Bob sorts the list in
increasing order of the task runtimes and then performs list scheduling. Who do you expect to
finish first? Please explain why.

	In order to finish the n tasks as quickly as possible, assuming the m processors run at the same speed for both Alice and Bob,
	it would make sense that the computer with less processors inactive at a time would work more efficiently and finish first. So,
	I would expect Alice to finish first. The reason is that the later the computer runs, when the number of remaining tasks is less
	than m, there would be processors that would not be utilized. If longer tasks were processed later than shorter ones, as in Bob's
	case, then the processors would likely not be utilized for longer periods of time as opposed to Alice's case, which is of course
	less efficient (as the rate of work being done over time would be lower).


