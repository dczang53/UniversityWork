# Dennis Zang (704766877), Assignment 3

# -*- coding: utf-8 -*-
"""Fall2020_CS146_HW3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hAgSdduwM9tzuZwjSi2k2--DCD5OteXY
"""

import os
import numpy as np
import matplotlib.pyplot as plt
import torch
from torch.utils.data import TensorDataset, DataLoader
from PIL import Image

# To add your own Drive Run this cell.
from google.colab import drive
drive.mount('/content/drive')

######################################################################
# OneLayerNetwork
######################################################################

class OneLayerNetwork(torch.nn.Module):
    def __init__(self):
        super(OneLayerNetwork, self).__init__()

        ### ========== TODO : START ========== ###
        ### part d: implement OneLayerNetwork with torch.nn.Linear
        self.W = torch.nn.Linear(784, 3)    # single linear layer in NN
        ### ========== TODO : END ========== ###

    def forward(self, x):
        # x.shape = (n_batch, n_features)

        ### ========== TODO : START ========== ###
        ### part d: implement the foward function
        outputs = self.W(x)     # calculate Wx (sigma function dealt implicitly by optimizer)
        ### ========== TODO : END ========== ###
        return outputs

######################################################################
# TwoLayerNetwork
######################################################################

class TwoLayerNetwork(torch.nn.Module):
    def __init__(self):
        super(TwoLayerNetwork, self).__init__()
        ### ========== TODO : START ========== ###
        ### part g: implement TwoLayerNetwork with torch.nn.Linear
        self.W1 = torch.nn.Linear(784, 400)     # first "linear" layer in NN
        self.W2 = torch.nn.Linear(400, 3)       # second linear layer in NN
        ### ========== TODO : END ========== ###

    def forward(self, x):
        # x.shape = (n_batch, n_features)

        ### ========== TODO : START ========== ###
        ### part g: implement the foward function
        outputs = self.W2(torch.sigmoid(self.W1(x)))    # calculate W2(sigma(W1x)) (sigma function of second layer dealt implicitly by optimizer)
        ### ========== TODO : END ========== ###
        return outputs

# load data from csv
# X.shape = (n_examples, n_features), y.shape = (n_examples, )
def load_data(filename):
    data = np.loadtxt(filename)
    y = data[:, 0].astype(int)
    X = data[:, 1:].astype(np.float32) / 255
    return X, y

# plot one example
# x.shape = (features, )
def plot_img(x):
    x = x.reshape(28, 28)
    img = Image.fromarray(x*255)
    plt.figure()
    plt.imshow(img)
    return

def evaluate_loss(model, criterion, dataloader):
    model.eval()
    total_loss = 0.0
    for batch_X, batch_y in dataloader:
        outputs = model(batch_X)
        loss = criterion(outputs, batch_y)
        total_loss += loss.item()
        
    return total_loss / len(dataloader)

def evaluate_acc(model, dataloader):
    model.eval()
    total_acc = 0.0
    for batch_X, batch_y in dataloader:
        outputs = model(batch_X)
        predictions = torch.argmax(outputs, dim=1)
        total_acc += (predictions==batch_y).sum()
        
    return total_acc / len(dataloader.dataset)

def train(model, criterion, optimizer, train_loader, valid_loader):
    train_loss_list = []
    valid_loss_list = []
    train_acc_list = []
    valid_acc_list = []
    for epoch in range(1, 31):
        model.train()
        for batch_X, batch_y in train_loader:
            ### ========== TODO : START ========== ###
            ### part f: implement the training process
            optimizer.zero_grad()               # initialize / zero out the gradient
            predY = model.forward(batch_X)      # calculate the output predicted by the current model
            loss = criterion(predY, batch_y)    # calculate the loss
            loss.backward()                     # calculate the gradient
            optimizer.step()                    # apply the gradient for optimization
            ### ========== TODO : END ========== ###
            
        train_loss = evaluate_loss(model, criterion, train_loader)
        valid_loss = evaluate_loss(model, criterion, valid_loader)
        train_acc = evaluate_acc(model, train_loader)
        valid_acc = evaluate_acc(model, valid_loader)
        train_loss_list.append(train_loss)
        valid_loss_list.append(valid_loss)
        train_acc_list.append(train_acc)
        valid_acc_list.append(valid_acc)

        print(f"| epoch {epoch:2d} | train loss {train_loss:.6f} | train acc {train_acc:.6f} | valid loss {valid_loss:.6f} | valid acc {valid_acc:.6f} |")

    return train_loss_list, valid_loss_list, train_acc_list, valid_acc_list

######################################################################
# main
######################################################################

def main():

    # fix random seed
    np.random.seed(0)
    torch.manual_seed(0)

    # load data with correct file path

    ### ========== TODO : START ========== ###
    data_directory_path =  "/content/drive/My Drive/Fall2020-CS146-HW3"
    ### ========== TODO : END ========== ###

    # X.shape = (n_examples, n_features)
    # y.shape = (n_examples, )
    X_train, y_train = load_data(os.path.join(data_directory_path, "hw3_train.csv"))
    X_valid, y_valid = load_data(os.path.join(data_directory_path, "hw3_valid.csv"))
    X_test, y_test = load_data(os.path.join(data_directory_path, "hw3_test.csv"))

    ### ========== TODO : START ========== ###
    ### part a: print out three training images with different labels
    plot_img(X_train[0])  # 1
    plot_img(X_train[1])  # 0
    plot_img(X_train[7])  # 2
    ### ========== TODO : END ========== ###

    print("Data preparation...")

    ### ========== TODO : START ========== ###
    ### part b: convert numpy arrays to tensors
    XtrainTensors = torch.from_numpy(X_train)
    ytrainTensors = torch.from_numpy(y_train)
    XvalidTensors = torch.from_numpy(X_valid)
    yvalidTensors = torch.from_numpy(y_valid)
    XtestTensors = torch.from_numpy(X_test)
    ytestTensors = torch.from_numpy(y_test)
    ### ========== TODO : END ========== ###

    ### ========== TODO : START ========== ###
    ### part c: prepare dataloaders for training, validation, and testing
    ###         we expect to get a batch of pairs (x_n, y_n) from the dataloader
    train_loader = DataLoader(TensorDataset(XtrainTensors, ytrainTensors), batch_size=10, shuffle=True)   # not sure if shuffle is needed
    valid_loader = DataLoader(TensorDataset(XvalidTensors, yvalidTensors), batch_size=10, shuffle=True)
    test_loader = DataLoader(TensorDataset(XtestTensors, ytestTensors), batch_size=10, shuffle=True)
    '''
    # just testing out dataloader for personal reference; pls ignore this
    for batch_ndx, sample in enumerate(train_loader):
      print(len(sample[0]))     # 10
      print(len(sample[0][0]))  # 784
      print(len(sample[1]))     # 10
      print(sample[1])          # 1 x 10 tensor/array
      print()
    '''
    ### ========== TODO : END ========== ###

    ### ========== TODO : START ========== ###
    ### part e: prepare OneLayerNetwork, criterion, and optimizer
    model_one = OneLayerNetwork()
    criterion = torch.nn.CrossEntropyLoss()
    optimizer = torch.optim.SGD(model_one.parameters(), lr=0.0005)
    # print(list(model_one.parameters()))   # just to check parameter list here
    ### ========== TODO : END ========== ###

    print("Start training OneLayerNetwork...")
    results_one = train(model_one, criterion, optimizer, train_loader, valid_loader)
    print("Done!")

    ### ========== TODO : START ========== ###
    ### part h: prepare TwoLayerNetwork, criterion, and optimizer
    model_two = TwoLayerNetwork()
    criterion = torch.nn.CrossEntropyLoss()
    optimizer = torch.optim.SGD(model_two.parameters(), lr=0.0005)
    # print(list(model_two.parameters()))   # just to check parameter list here
    ### ========== TODO : END ========== ###

    print("Start training TwoLayerNetwork...")
    results_two = train(model_two, criterion, optimizer, train_loader, valid_loader)
    print("Done!")

    one_train_loss, one_valid_loss, one_train_acc, one_valid_acc = results_one
    two_train_loss, two_valid_loss, two_train_acc, two_valid_acc = results_two

    ### ========== TODO : START ========== ###
    ### part i: generate a plot to comare one_train_loss, one_valid_loss, two_train_loss, two_valid_loss
    plotX = [i for i in range(1, 31)]
    fig = plt.figure(figsize=(16,4))      # generatine figure with 4 subplots
    ax1 = fig.add_subplot(141)
    ax2 = fig.add_subplot(142)
    ax3 = fig.add_subplot(143)
    ax4 = fig.add_subplot(144)
    ax1.plot(plotX, one_train_loss)
    ax1.title.set_text("one_train_loss")
    ax2.plot(plotX, one_valid_loss)
    ax2.title.set_text("one_valid_loss")
    ax3.plot(plotX, two_train_loss)
    ax3.title.set_text("two_train_loss")
    ax4.plot(plotX, two_valid_loss)
    ax4.title.set_text("two_valid_loss")
    plt.show()
    ### ========== TODO : END ========== ###

    ### ========== TODO : START ========== ###
    ### part j: generate a plot to comare one_train_acc, one_valid_acc, two_train_acc, two_valid_acc
    fig2 = plt.figure(figsize=(16,4))     # generatine figure with 4 subplots
    ax1 = fig2.add_subplot(141)
    ax2 = fig2.add_subplot(142)
    ax3 = fig2.add_subplot(143)
    ax4 = fig2.add_subplot(144)
    ax1.plot(plotX, one_train_acc)
    ax1.title.set_text("one_train_acc")
    ax2.plot(plotX, one_valid_acc)
    ax2.title.set_text("one_valid_acc")
    ax3.plot(plotX, two_train_acc)
    ax3.title.set_text("two_train_acc")
    ax4.plot(plotX, two_valid_acc)
    ax4.title.set_text("two_valid_acc")
    plt.show()
    ### ========== TODO : END ========== ##

    ### ========== TODO : START ========== ###
    ### part k: calculate the test accuracy
    print(f"one-layer test accuracy: {evaluate_acc(model_one, test_loader):.6f}")
    print(f"two-layer test accuracy: {evaluate_acc(model_two, test_loader):.6f}")
    ### ========== TODO : END ========== ###

    ### ========== TODO : START ========== ###
    ### part l: replace the SGD optimizer with the Adam optimizer and do the experiments again
    # pretty much a repeat of what is done above
    model_one = OneLayerNetwork()
    criterion = torch.nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model_one.parameters(), lr=0.0005)
    results_one = train(model_one, criterion, optimizer, train_loader, valid_loader)
    print("Done!")
    model_two = TwoLayerNetwork()
    criterion = torch.nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model_two.parameters(), lr=0.0005)
    results_two = train(model_two, criterion, optimizer, train_loader, valid_loader)
    print("Done!")
    one_train_loss, one_valid_loss, one_train_acc, one_valid_acc = results_one
    two_train_loss, two_valid_loss, two_train_acc, two_valid_acc = results_two
    plotX = [i for i in range(1, 31)]
    fig3 = plt.figure(figsize=(16,4))
    ax1 = fig3.add_subplot(141)
    ax2 = fig3.add_subplot(142)
    ax3 = fig3.add_subplot(143)
    ax4 = fig3.add_subplot(144)
    ax1.plot(plotX, one_train_loss)
    ax1.title.set_text("one_train_loss")
    ax2.plot(plotX, one_valid_loss)
    ax2.title.set_text("one_valid_loss")
    ax3.plot(plotX, two_train_loss)
    ax3.title.set_text("two_train_loss")
    ax4.plot(plotX, two_valid_loss)
    ax4.title.set_text("two_valid_loss")
    plt.show()
    fig4 = plt.figure(figsize=(16,4))
    ax1 = fig4.add_subplot(141)
    ax2 = fig4.add_subplot(142)
    ax3 = fig4.add_subplot(143)
    ax4 = fig4.add_subplot(144)
    ax1.plot(plotX, one_train_acc)
    ax1.title.set_text("one_train_acc")
    ax2.plot(plotX, one_valid_acc)
    ax2.title.set_text("one_valid_acc")
    ax3.plot(plotX, two_train_acc)
    ax3.title.set_text("two_train_acc")
    ax4.plot(plotX, two_valid_acc)
    ax4.title.set_text("two_valid_acc")
    plt.show()
    print(f"one-layer test accuracy: {evaluate_acc(model_one, test_loader):.6f}")
    print(f"two-layer test accuracy: {evaluate_acc(model_two, test_loader):.6f}")
    ### ========== TODO : END ========== ###



if __name__ == "__main__":
    main()
