# # Dennis Zang, 704766877, Homework 4
# -*- coding: utf-8 -*-
"""Fall2020_CS146_HW4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TNoZ7JSu9sfESzYV8WNvW3dK818DTtJK
"""

# To add your own Drive Run this cell.
from google.colab import drive
drive.mount('/content/drive')

"""
Author      : Zeyuan Chen, adapted from Yi-Chieh Wu, Sriram Sankararman
Description : Twitter
"""

import warnings
warnings.filterwarnings('ignore')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt


from sklearn import metrics 
from sklearn.svm import SVC
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.mixture import GaussianMixture
from sklearn.metrics.cluster import adjusted_rand_score

import pandas as pd
### ========== TODO : START ========== ###
# append you own path to the tweeter_df.txt file after "/content/drive/My Drive/"
# i.e. "/content/drive/My Drive/CM146HW4/tweets_df.txt"
tweets_df = pd.read_csv("/content/drive/My Drive/Fall2020-CS146-HW4/tweets_df.txt", index_col = 0)
### ========== TODO : END ========== ###

X = tweets_df.values[:,:-2]
y = tweets_df.values[:, -2]
movies = tweets_df.values[:, -1]

train_idx = np.where((movies == 1) | (movies == 3))[0]
dev_idx   = np.where(movies == 2)[0]
test_idx  = np.where(movies == 0)[0]

X_train, X_dev, X_test = X[train_idx,], X[dev_idx,], X[test_idx,]
y_train, y_dev, y_test = y[train_idx,], y[dev_idx,], y[test_idx,]

### ========== TODO : START ========== ###
# part 4.1a: show the train and development set f1-score when C is set to 10^-3 10^-2 10^-1 1, 10, 100, 1000
#            This can typically be done in 20 lines or so
plotX = []        # variables for plotting
plotYTrain = []
plotYDev = []
C = 0.001         # iterating values of C
while C <= 1000:
  plotX.append(C)
  linearSVM = SVC(C=C, kernel="linear")
  linearSVM.fit(X_train, y_train)                       # train SVM with train set
  y_trainPred = linearSVM.predict(X_train)              # predict on train set
  trainF1Score = metrics.f1_score(y_train, y_trainPred) # calculate the f1 score for train set
  plotYTrain.append(trainF1Score)
  y_devPred = linearSVM.predict(X_dev)                  # predict of dev set
  devF1Score = metrics.f1_score(y_dev, y_devPred)       # calculate the f1 score for dev set
  plotYDev.append(devF1Score)
  C = C * 10;
fig = plt.figure(figsize=(8,4))           # plotting here
ax1 = fig.add_subplot(121)
ax2 = fig.add_subplot(122)
ax1.set_xscale('log')
ax1.set_yscale('linear')
ax2.set_xscale('log')
ax2.set_yscale('linear')
ax1.plot(plotX, plotYTrain)
ax1.title.set_text("Train F1 Score")
ax2.plot(plotX, plotYDev)
ax2.title.set_text("Dev F1 Score")
plt.show()
# print(plotYDev)                         # to better observe the slight maximum on dev f1 scores
### ========== TODO : END ========== ###

### ========== TODO : START ========== ###
# part 4.1b: select the best model based on development set f1-score 
#            retrain the model on train and dev set
#            test the final model on the test set
#            This can typically be done in 5 lines or so
linearSVM = SVC(C=1, kernel="linear")
linearSVM.fit(np.concatenate((X_train, X_dev), axis=0), np.concatenate((y_train, y_dev), axis=0)) # train SVM with train set appended with dev test
y_testPred = linearSVM.predict(X_test)                                                            # predict on test set
testF1Score = metrics.f1_score(y_test, y_testPred)                                                # calculate the f1 score for test set
print(f"testF1Score: {testF1Score}")
### ========== TODO : END ========== ###

def plot_scatter(embedding_2d, labels, show = True, save_as = None, title = None):
    """
    Visualize 2D data

    Parameters
    --------------------
        embedding_2d   -- numpy array of shape (n,2) samples
        labels         -- numpy array of shape (n,) labels
        show           -- optional boolean indicator on if display the visualziation, default set to True 
        save_as        -- optional string indicating where we should save the figure, default set to None
        title          -- optional string indicating what should be the title, default set to None
    --------------------
        None    
    """
    
    plt.scatter(embedding_2d[:,0], embedding_2d[:,1], c = labels) 
    if title is not None:
        plt.title(title)
    if save_as is not None:
        plt.savefig(save_as)
    if show:
        plt.show()

pca = PCA(n_components=2)
pca.fit(X)
X_embedding = pca.transform(X)

### ========== TODO : START ========== ###
# part 4.2a: visualize the embedding. First color the dots by positive or negative review, then by movies 
#            This can typically be done in 2 lines or so
plot_scatter(X_embedding, y, title="Positive and Negative Clusters")
plot_scatter(X_embedding, movies, title="Movie Clusters")
### ========== TODO : END ========== ###

### ========== TODO : START ========== ###
# part 4.2b: First color the dots by Kmeans with 4 components, random initialization, 1 iteration, random_state = 2
#            then by GMM with 4 components, random starting point, 1 iteration and random_state = 0
#            report the adjusted rand score for both
#            This can typically be done in 10 lines or so
k4means = KMeans(n_clusters=4, init='random', n_init=1, random_state=2)                     # initialize KMeans classifier
k4means.fit(X_embedding)                                                                    # fit to X_embedding
k4meansPred = k4means.predict(X_embedding)                                                  # get predictions on training set
plot_scatter(X_embedding, k4meansPred, title="K-Means Prediction")
print(f"K-Means Adjusted Rand Score: {metrics.adjusted_rand_score(k4meansPred, movies)}")
gmm = GaussianMixture(n_components=4, random_state=0, init_params='random')                 # initialize Gaussian Misxture classifier
gmm.fit(X_embedding)                                                                        # fit to X_embedding
gmmPred = gmm.predict(X_embedding)                                                          # get predictions on training set
plot_scatter(X_embedding, gmmPred, title="Gaussian Mixture Prediction")
print(f"Gaussian Mixture Adjusted Rand Score: {metrics.adjusted_rand_score(gmmPred, movies)}")
### ========== TODO : END ========== ###

### ========== TODO : START ========== ###
# part 4.2c: First color the dots by Kmeans with 4 components, random initialization, 100 iterations, random_state = 2
#            then by GMM with 4 components, random starting point, 100 iterations and random_state = 0
#            report then adjusted rand score for both
#            This can typically be done in 10 lines or so
k4means = KMeans(n_clusters=4, init='random', n_init=100, random_state=2)                     # initialize KMeans classifier
k4means.fit(X_embedding)                                                                      # fit to X_embedding
k4meansPred = k4means.predict(X_embedding)                                                    # get predictions on training set
plot_scatter(X_embedding, k4meansPred, title="K-Means Prediction")
print(f"K-Means Adjusted Rand Score: {metrics.adjusted_rand_score(k4meansPred, movies)}")
gmm = GaussianMixture(n_components=4, random_state=0, init_params='random', n_init=100)       # initialize Gaussian Misxture classifier
gmm.fit(X_embedding)                                                                          # fit to X_embedding
gmmPred = gmm.predict(X_embedding)                                                            # get predictions on training set
plot_scatter(X_embedding, gmmPred, title="Gaussian Mixture Prediction")
print(f"Gaussian Mixture Adjusted Rand Score: {metrics.adjusted_rand_score(gmmPred, movies)}")
### ========== TODO : END ========== ###
